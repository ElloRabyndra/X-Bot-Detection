{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d72a600",
   "metadata": {},
   "source": [
    "### ***DATASET PREPARATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d471472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING STRATIFIED SAMPLE\n",
      "======================================================================\n",
      "Original dataset: 1,000,000 users\n",
      "\n",
      "Target sample: 20,000 users\n",
      "  - Bots: 2,800\n",
      "  - Humans: 17,200\n",
      "\n",
      "âœ… Saved to 'sampled_labels.csv'\n",
      "\n",
      "âœ… 20,000 users sampled\n",
      "\n",
      "======================================================================\n",
      "FILTERING TWEETS FOR SAMPLED USERS\n",
      "======================================================================\n",
      "Processing edge.csv in chunks...\n",
      "  Processed 1,000,000 edges...\n",
      "  Processed 2,000,000 edges...\n",
      "  Processed 3,000,000 edges...\n",
      "  Processed 4,000,000 edges...\n",
      "  Processed 5,000,000 edges...\n",
      "  Processed 6,000,000 edges...\n",
      "  Processed 7,000,000 edges...\n",
      "  Processed 8,000,000 edges...\n",
      "  Processed 9,000,000 edges...\n",
      "  Processed 10,000,000 edges...\n",
      "  Processed 11,000,000 edges...\n",
      "  Processed 12,000,000 edges...\n",
      "  Processed 13,000,000 edges...\n",
      "  Processed 14,000,000 edges...\n",
      "  Processed 15,000,000 edges...\n",
      "  Processed 16,000,000 edges...\n",
      "  Processed 17,000,000 edges...\n",
      "  Processed 18,000,000 edges...\n",
      "  Processed 19,000,000 edges...\n",
      "  Processed 20,000,000 edges...\n",
      "  Processed 21,000,000 edges...\n",
      "  Processed 22,000,000 edges...\n",
      "  Processed 23,000,000 edges...\n",
      "  Processed 24,000,000 edges...\n",
      "  Processed 25,000,000 edges...\n",
      "  Processed 26,000,000 edges...\n",
      "  Processed 27,000,000 edges...\n",
      "  Processed 28,000,000 edges...\n",
      "  Processed 29,000,000 edges...\n",
      "  Processed 30,000,000 edges...\n",
      "  Processed 31,000,000 edges...\n",
      "  Processed 32,000,000 edges...\n",
      "  Processed 33,000,000 edges...\n",
      "  Processed 34,000,000 edges...\n",
      "  Processed 35,000,000 edges...\n",
      "  Processed 36,000,000 edges...\n",
      "  Processed 37,000,000 edges...\n",
      "  Processed 38,000,000 edges...\n",
      "  Processed 39,000,000 edges...\n",
      "  Processed 40,000,000 edges...\n",
      "  Processed 41,000,000 edges...\n",
      "  Processed 42,000,000 edges...\n",
      "  Processed 43,000,000 edges...\n",
      "  Processed 44,000,000 edges...\n",
      "  Processed 45,000,000 edges...\n",
      "  Processed 46,000,000 edges...\n",
      "  Processed 47,000,000 edges...\n",
      "  Processed 48,000,000 edges...\n",
      "  Processed 49,000,000 edges...\n",
      "  Processed 50,000,000 edges...\n",
      "  Processed 51,000,000 edges...\n",
      "  Processed 52,000,000 edges...\n",
      "  Processed 53,000,000 edges...\n",
      "  Processed 54,000,000 edges...\n",
      "  Processed 55,000,000 edges...\n",
      "  Processed 56,000,000 edges...\n",
      "  Processed 57,000,000 edges...\n",
      "  Processed 58,000,000 edges...\n",
      "  Processed 59,000,000 edges...\n",
      "  Processed 60,000,000 edges...\n",
      "  Processed 61,000,000 edges...\n",
      "  Processed 62,000,000 edges...\n",
      "  Processed 63,000,000 edges...\n",
      "  Processed 64,000,000 edges...\n",
      "  Processed 65,000,000 edges...\n",
      "  Processed 66,000,000 edges...\n",
      "  Processed 67,000,000 edges...\n",
      "  Processed 68,000,000 edges...\n",
      "  Processed 69,000,000 edges...\n",
      "  Processed 70,000,000 edges...\n",
      "  Processed 71,000,000 edges...\n",
      "  Processed 72,000,000 edges...\n",
      "  Processed 73,000,000 edges...\n",
      "  Processed 74,000,000 edges...\n",
      "  Processed 75,000,000 edges...\n",
      "  Processed 76,000,000 edges...\n",
      "  Processed 77,000,000 edges...\n",
      "  Processed 78,000,000 edges...\n",
      "  Processed 79,000,000 edges...\n",
      "  Processed 80,000,000 edges...\n",
      "  Processed 81,000,000 edges...\n",
      "  Processed 82,000,000 edges...\n",
      "  Processed 83,000,000 edges...\n",
      "  Processed 84,000,000 edges...\n",
      "  Processed 85,000,000 edges...\n",
      "  Processed 86,000,000 edges...\n",
      "  Processed 87,000,000 edges...\n",
      "  Processed 88,000,000 edges...\n",
      "  Processed 89,000,000 edges...\n",
      "  Processed 90,000,000 edges...\n",
      "  Processed 91,000,000 edges...\n",
      "  Processed 92,000,000 edges...\n",
      "  Processed 93,000,000 edges...\n",
      "  Processed 94,000,000 edges...\n",
      "  Processed 95,000,000 edges...\n",
      "  Processed 96,000,000 edges...\n",
      "  Processed 97,000,000 edges...\n",
      "  Processed 98,000,000 edges...\n",
      "  Processed 99,000,000 edges...\n",
      "  Processed 100,000,000 edges...\n",
      "  Processed 101,000,000 edges...\n",
      "  Processed 102,000,000 edges...\n",
      "  Processed 103,000,000 edges...\n",
      "  Processed 104,000,000 edges...\n",
      "  Processed 105,000,000 edges...\n",
      "  Processed 106,000,000 edges...\n",
      "  Processed 107,000,000 edges...\n",
      "  Processed 108,000,000 edges...\n",
      "  Processed 109,000,000 edges...\n",
      "  Processed 110,000,000 edges...\n",
      "  Processed 111,000,000 edges...\n",
      "  Processed 112,000,000 edges...\n",
      "  Processed 113,000,000 edges...\n",
      "  Processed 114,000,000 edges...\n",
      "  Processed 115,000,000 edges...\n",
      "  Processed 116,000,000 edges...\n",
      "  Processed 117,000,000 edges...\n",
      "  Processed 118,000,000 edges...\n",
      "  Processed 119,000,000 edges...\n",
      "  Processed 120,000,000 edges...\n",
      "  Processed 121,000,000 edges...\n",
      "  Processed 122,000,000 edges...\n",
      "  Processed 123,000,000 edges...\n",
      "  Processed 124,000,000 edges...\n",
      "  Processed 125,000,000 edges...\n",
      "  Processed 126,000,000 edges...\n",
      "  Processed 127,000,000 edges...\n",
      "  Processed 128,000,000 edges...\n",
      "  Processed 129,000,000 edges...\n",
      "  Processed 130,000,000 edges...\n",
      "  Processed 131,000,000 edges...\n",
      "  Processed 132,000,000 edges...\n",
      "  Processed 133,000,000 edges...\n",
      "  Processed 134,000,000 edges...\n",
      "  Processed 135,000,000 edges...\n",
      "  Processed 136,000,000 edges...\n",
      "  Processed 137,000,000 edges...\n",
      "  Processed 138,000,000 edges...\n",
      "  Processed 139,000,000 edges...\n",
      "  Processed 140,000,000 edges...\n",
      "  Processed 141,000,000 edges...\n",
      "  Processed 142,000,000 edges...\n",
      "  Processed 143,000,000 edges...\n",
      "  Processed 144,000,000 edges...\n",
      "  Processed 145,000,000 edges...\n",
      "  Processed 146,000,000 edges...\n",
      "  Processed 147,000,000 edges...\n",
      "  Processed 148,000,000 edges...\n",
      "  Processed 149,000,000 edges...\n",
      "  Processed 150,000,000 edges...\n",
      "  Processed 151,000,000 edges...\n",
      "  Processed 152,000,000 edges...\n",
      "  Processed 153,000,000 edges...\n",
      "  Processed 154,000,000 edges...\n",
      "  Processed 155,000,000 edges...\n",
      "  Processed 156,000,000 edges...\n",
      "  Processed 157,000,000 edges...\n",
      "  Processed 158,000,000 edges...\n",
      "  Processed 159,000,000 edges...\n",
      "  Processed 160,000,000 edges...\n",
      "  Processed 161,000,000 edges...\n",
      "  Processed 162,000,000 edges...\n",
      "  Processed 163,000,000 edges...\n",
      "  Processed 164,000,000 edges...\n",
      "  Processed 165,000,000 edges...\n",
      "  Processed 166,000,000 edges...\n",
      "  Processed 167,000,000 edges...\n",
      "  Processed 168,000,000 edges...\n",
      "  Processed 169,000,000 edges...\n",
      "  Processed 170,000,000 edges...\n",
      "\n",
      "âœ… Filtered tweets: 1,784,030\n",
      "âœ… Saved to 'sampled_edges.csv'\n",
      "\n",
      "======================================================================\n",
      "TWEETS PER USER DISTRIBUTION (SAMPLED)\n",
      "======================================================================\n",
      "Users with tweets: 18,749\n",
      "\n",
      "Statistics:\n",
      "count    18749.000000\n",
      "mean        95.153342\n",
      "std        118.058141\n",
      "min          1.000000\n",
      "25%         40.000000\n",
      "50%         54.000000\n",
      "75%        115.000000\n",
      "max       4761.000000\n",
      "dtype: float64\n",
      "\n",
      "ðŸ“Œ Users with â‰¥2 tweets: 18,495 (98.6%)\n",
      "ðŸ“Œ Users with â‰¥5 tweets: 18,190 (97.0%)\n",
      "ðŸ“Œ Users with â‰¥10 tweets: 17,832 (95.1%)\n",
      "ðŸ“Œ Users with 1 tweet: 254 (1.4%)\n",
      "\n",
      "======================================================================\n",
      "FINAL USABLE DATASET\n",
      "======================================================================\n",
      "âœ… Final users (â‰¥2 tweets): 18,495\n",
      "  - Bots: 2,277\n",
      "  - Humans: 16,218\n",
      "\n",
      "âœ… Saved to 'final_labels.csv'\n",
      "âœ… Total tweets to extract: 1,783,776\n",
      "âœ… Saved tweet IDs to 'final_tweet_ids.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: STRATIFIED SAMPLING DARI LABELS\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING STRATIFIED SAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load labels\n",
    "labels = pd.read_csv('x_dataset/label.csv')\n",
    "print(f\"Original dataset: {len(labels):,} users\")\n",
    "\n",
    "# Target sample size\n",
    "SAMPLE_SIZE = 20000  # Adjust: 10000, 20000, or 50000\n",
    "\n",
    "# Stratified sampling (maintain bot:human ratio)\n",
    "bot_ratio = 0.14  # 14% bots\n",
    "n_bots = int(SAMPLE_SIZE * bot_ratio)\n",
    "n_humans = SAMPLE_SIZE - n_bots\n",
    "\n",
    "print(f\"\\nTarget sample: {SAMPLE_SIZE:,} users\")\n",
    "print(f\"  - Bots: {n_bots:,}\")\n",
    "print(f\"  - Humans: {n_humans:,}\")\n",
    "\n",
    "# Sample\n",
    "bots = labels[labels['label'] == 'bot'].sample(n=n_bots, random_state=42)\n",
    "humans = labels[labels['label'] == 'human'].sample(n=n_humans, random_state=42)\n",
    "sampled_labels = pd.concat([bots, humans]).reset_index(drop=True)\n",
    "\n",
    "# Save sampled labels\n",
    "sampled_labels.to_csv('final_x_dataset/sampled_labels.csv', index=False)\n",
    "print(f\"\\nâœ… Saved to 'sampled_labels.csv'\")\n",
    "\n",
    "# Get sampled user IDs\n",
    "sampled_user_ids = set(sampled_labels['id'].values)\n",
    "print(f\"\\nâœ… {len(sampled_user_ids):,} users sampled\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: FILTER EDGES (POST RELATIONS) FOR SAMPLED USERS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILTERING TWEETS FOR SAMPLED USERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load edges in chunks to avoid memory issues\n",
    "chunk_size = 100000\n",
    "filtered_edges = []\n",
    "\n",
    "print(\"Processing edge.csv in chunks...\")\n",
    "for i, chunk in enumerate(pd.read_csv('x_dataset/edge.csv', \n",
    "                                       names=['source_id', 'relation', 'target_id'],\n",
    "                                       chunksize=chunk_size)):\n",
    "    # Filter post relations for sampled users\n",
    "    post_chunk = chunk[(chunk['relation'] == 'post') & \n",
    "                       (chunk['source_id'].isin(sampled_user_ids))]\n",
    "    filtered_edges.append(post_chunk)\n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"  Processed {(i+1)*chunk_size:,} edges...\")\n",
    "\n",
    "# Combine filtered edges\n",
    "sampled_edges = pd.concat(filtered_edges).reset_index(drop=True)\n",
    "sampled_edges.to_csv('final_x_dataset/sampled_edges.csv', index=False)\n",
    "print(f\"\\nâœ… Filtered tweets: {len(sampled_edges):,}\")\n",
    "print(f\"âœ… Saved to 'sampled_edges.csv'\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: TWEETS PER USER STATISTICS (SAMPLED)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TWEETS PER USER DISTRIBUTION (SAMPLED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tweets_per_user = sampled_edges.groupby('source_id').size()\n",
    "print(f\"Users with tweets: {len(tweets_per_user):,}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(tweets_per_user.describe())\n",
    "\n",
    "print(f\"\\nðŸ“Œ Users with â‰¥2 tweets: {(tweets_per_user >= 2).sum():,} ({(tweets_per_user >= 2).sum()/len(tweets_per_user)*100:.1f}%)\")\n",
    "print(f\"ðŸ“Œ Users with â‰¥5 tweets: {(tweets_per_user >= 5).sum():,} ({(tweets_per_user >= 5).sum()/len(tweets_per_user)*100:.1f}%)\")\n",
    "print(f\"ðŸ“Œ Users with â‰¥10 tweets: {(tweets_per_user >= 10).sum():,} ({(tweets_per_user >= 10).sum()/len(tweets_per_user)*100:.1f}%)\")\n",
    "print(f\"ðŸ“Œ Users with 1 tweet: {(tweets_per_user == 1).sum():,} ({(tweets_per_user == 1).sum()/len(tweets_per_user)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: FINAL USABLE DATASET\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL USABLE DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Users with â‰¥2 tweets (can calculate TIE)\n",
    "users_with_min_tweets = set(tweets_per_user[tweets_per_user >= 2].index)\n",
    "final_user_ids = users_with_min_tweets.intersection(sampled_user_ids)\n",
    "\n",
    "final_labels = sampled_labels[sampled_labels['id'].isin(final_user_ids)]\n",
    "print(f\"âœ… Final users (â‰¥2 tweets): {len(final_user_ids):,}\")\n",
    "print(f\"  - Bots: {(final_labels['label'] == 'bot').sum():,}\")\n",
    "print(f\"  - Humans: {(final_labels['label'] == 'human').sum():,}\")\n",
    "\n",
    "# Save final user list\n",
    "final_labels.to_csv('final_x_dataset/final_labels.csv', index=False)\n",
    "print(f\"\\nâœ… Saved to 'final_labels.csv'\")\n",
    "\n",
    "# Save final tweet IDs for extraction\n",
    "final_edges = sampled_edges[sampled_edges['source_id'].isin(final_user_ids)]\n",
    "final_tweet_ids = set(final_edges['target_id'].values)\n",
    "print(f\"âœ… Total tweets to extract: {len(final_tweet_ids):,}\")\n",
    "\n",
    "# Save tweet IDs for next step\n",
    "with open('final_tweet_ids.txt', 'w') as f:\n",
    "    for tid in final_tweet_ids:\n",
    "        f.write(f\"{tid}\\n\")\n",
    "print(f\"âœ… Saved tweet IDs to 'final_tweet_ids.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa588deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING ULTRA-SMALL SAMPLE (MEMORY-SAFE)\n",
      "======================================================================\n",
      "Original dataset: 1,000,000 users\n",
      "\n",
      "Target sample: 2,000 users\n",
      "  - Bots: 280\n",
      "  - Humans: 1,720\n",
      "\n",
      "âœ… Saved to 'sampled_labels_small.csv'\n",
      "\n",
      "======================================================================\n",
      "FILTERING TWEETS FOR SMALL SAMPLE\n",
      "======================================================================\n",
      "  Processed 1,000,000 edges...\n",
      "  Processed 2,000,000 edges...\n",
      "  Processed 3,000,000 edges...\n",
      "  Processed 4,000,000 edges...\n",
      "  Processed 5,000,000 edges...\n",
      "  Processed 6,000,000 edges...\n",
      "  Processed 7,000,000 edges...\n",
      "  Processed 8,000,000 edges...\n",
      "  Processed 9,000,000 edges...\n",
      "  Processed 10,000,000 edges...\n",
      "  Processed 11,000,000 edges...\n",
      "  Processed 12,000,000 edges...\n",
      "  Processed 13,000,000 edges...\n",
      "  Processed 14,000,000 edges...\n",
      "  Processed 15,000,000 edges...\n",
      "  Processed 16,000,000 edges...\n",
      "  Processed 17,000,000 edges...\n",
      "  Processed 18,000,000 edges...\n",
      "  Processed 19,000,000 edges...\n",
      "  Processed 20,000,000 edges...\n",
      "  Processed 21,000,000 edges...\n",
      "  Processed 22,000,000 edges...\n",
      "  Processed 23,000,000 edges...\n",
      "  Processed 24,000,000 edges...\n",
      "  Processed 25,000,000 edges...\n",
      "  Processed 26,000,000 edges...\n",
      "  Processed 27,000,000 edges...\n",
      "  Processed 28,000,000 edges...\n",
      "  Processed 29,000,000 edges...\n",
      "  Processed 30,000,000 edges...\n",
      "  Processed 31,000,000 edges...\n",
      "  Processed 32,000,000 edges...\n",
      "  Processed 33,000,000 edges...\n",
      "  Processed 34,000,000 edges...\n",
      "  Processed 35,000,000 edges...\n",
      "  Processed 36,000,000 edges...\n",
      "  Processed 37,000,000 edges...\n",
      "  Processed 38,000,000 edges...\n",
      "  Processed 39,000,000 edges...\n",
      "  Processed 40,000,000 edges...\n",
      "  Processed 41,000,000 edges...\n",
      "  Processed 42,000,000 edges...\n",
      "  Processed 43,000,000 edges...\n",
      "  Processed 44,000,000 edges...\n",
      "  Processed 45,000,000 edges...\n",
      "  Processed 46,000,000 edges...\n",
      "  Processed 47,000,000 edges...\n",
      "  Processed 48,000,000 edges...\n",
      "  Processed 49,000,000 edges...\n",
      "  Processed 50,000,000 edges...\n",
      "  Processed 51,000,000 edges...\n",
      "  Processed 52,000,000 edges...\n",
      "  Processed 53,000,000 edges...\n",
      "  Processed 54,000,000 edges...\n",
      "  Processed 55,000,000 edges...\n",
      "  Processed 56,000,000 edges...\n",
      "  Processed 57,000,000 edges...\n",
      "  Processed 58,000,000 edges...\n",
      "  Processed 59,000,000 edges...\n",
      "  Processed 60,000,000 edges...\n",
      "  Processed 61,000,000 edges...\n",
      "  Processed 62,000,000 edges...\n",
      "  Processed 63,000,000 edges...\n",
      "  Processed 64,000,000 edges...\n",
      "  Processed 65,000,000 edges...\n",
      "  Processed 66,000,000 edges...\n",
      "  Processed 67,000,000 edges...\n",
      "  Processed 68,000,000 edges...\n",
      "  Processed 69,000,000 edges...\n",
      "  Processed 70,000,000 edges...\n",
      "  Processed 71,000,000 edges...\n",
      "  Processed 72,000,000 edges...\n",
      "  Processed 73,000,000 edges...\n",
      "  Processed 74,000,000 edges...\n",
      "  Processed 75,000,000 edges...\n",
      "  Processed 76,000,000 edges...\n",
      "  Processed 77,000,000 edges...\n",
      "  Processed 78,000,000 edges...\n",
      "  Processed 79,000,000 edges...\n",
      "  Processed 80,000,000 edges...\n",
      "  Processed 81,000,000 edges...\n",
      "  Processed 82,000,000 edges...\n",
      "  Processed 83,000,000 edges...\n",
      "  Processed 84,000,000 edges...\n",
      "  Processed 85,000,000 edges...\n",
      "  Processed 86,000,000 edges...\n",
      "  Processed 87,000,000 edges...\n",
      "  Processed 88,000,000 edges...\n",
      "  Processed 89,000,000 edges...\n",
      "  Processed 90,000,000 edges...\n",
      "  Processed 91,000,000 edges...\n",
      "  Processed 92,000,000 edges...\n",
      "  Processed 93,000,000 edges...\n",
      "  Processed 94,000,000 edges...\n",
      "  Processed 95,000,000 edges...\n",
      "  Processed 96,000,000 edges...\n",
      "  Processed 97,000,000 edges...\n",
      "  Processed 98,000,000 edges...\n",
      "  Processed 99,000,000 edges...\n",
      "  Processed 100,000,000 edges...\n",
      "  Processed 101,000,000 edges...\n",
      "  Processed 102,000,000 edges...\n",
      "  Processed 103,000,000 edges...\n",
      "  Processed 104,000,000 edges...\n",
      "  Processed 105,000,000 edges...\n",
      "  Processed 106,000,000 edges...\n",
      "  Processed 107,000,000 edges...\n",
      "  Processed 108,000,000 edges...\n",
      "  Processed 109,000,000 edges...\n",
      "  Processed 110,000,000 edges...\n",
      "  Processed 111,000,000 edges...\n",
      "  Processed 112,000,000 edges...\n",
      "  Processed 113,000,000 edges...\n",
      "  Processed 114,000,000 edges...\n",
      "  Processed 115,000,000 edges...\n",
      "  Processed 116,000,000 edges...\n",
      "  Processed 117,000,000 edges...\n",
      "  Processed 118,000,000 edges...\n",
      "  Processed 119,000,000 edges...\n",
      "  Processed 120,000,000 edges...\n",
      "  Processed 121,000,000 edges...\n",
      "  Processed 122,000,000 edges...\n",
      "  Processed 123,000,000 edges...\n",
      "  Processed 124,000,000 edges...\n",
      "  Processed 125,000,000 edges...\n",
      "  Processed 126,000,000 edges...\n",
      "  Processed 127,000,000 edges...\n",
      "  Processed 128,000,000 edges...\n",
      "  Processed 129,000,000 edges...\n",
      "  Processed 130,000,000 edges...\n",
      "  Processed 131,000,000 edges...\n",
      "  Processed 132,000,000 edges...\n",
      "  Processed 133,000,000 edges...\n",
      "  Processed 134,000,000 edges...\n",
      "  Processed 135,000,000 edges...\n",
      "  Processed 136,000,000 edges...\n",
      "  Processed 137,000,000 edges...\n",
      "  Processed 138,000,000 edges...\n",
      "  Processed 139,000,000 edges...\n",
      "  Processed 140,000,000 edges...\n",
      "  Processed 141,000,000 edges...\n",
      "  Processed 142,000,000 edges...\n",
      "  Processed 143,000,000 edges...\n",
      "  Processed 144,000,000 edges...\n",
      "  Processed 145,000,000 edges...\n",
      "  Processed 146,000,000 edges...\n",
      "  Processed 147,000,000 edges...\n",
      "  Processed 148,000,000 edges...\n",
      "  Processed 149,000,000 edges...\n",
      "  Processed 150,000,000 edges...\n",
      "  Processed 151,000,000 edges...\n",
      "  Processed 152,000,000 edges...\n",
      "  Processed 153,000,000 edges...\n",
      "  Processed 154,000,000 edges...\n",
      "  Processed 155,000,000 edges...\n",
      "  Processed 156,000,000 edges...\n",
      "  Processed 157,000,000 edges...\n",
      "  Processed 158,000,000 edges...\n",
      "  Processed 159,000,000 edges...\n",
      "  Processed 160,000,000 edges...\n",
      "  Processed 161,000,000 edges...\n",
      "  Processed 162,000,000 edges...\n",
      "  Processed 163,000,000 edges...\n",
      "  Processed 164,000,000 edges...\n",
      "  Processed 165,000,000 edges...\n",
      "  Processed 166,000,000 edges...\n",
      "  Processed 167,000,000 edges...\n",
      "  Processed 168,000,000 edges...\n",
      "  Processed 169,000,000 edges...\n",
      "  Processed 170,000,000 edges...\n",
      "\n",
      "âœ… Filtered tweets: 182,328\n",
      "\n",
      "ðŸ“Š Users with tweets: 1,868\n",
      "ðŸ“Š Users with â‰¥2 tweets: 1,842\n",
      "âœ… Saved 182,328 tweet IDs\n",
      "\n",
      "âœ… SMALL SAMPLE READY!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING ULTRA-SMALL SAMPLE (MEMORY-SAFE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# REDUCED SAMPLE: 2,000 USERS (instead of 20,000)\n",
    "# ============================================================\n",
    "SAMPLE_SIZE = 2000  # Much smaller!\n",
    "\n",
    "labels = pd.read_csv('x_dataset/label.csv')\n",
    "print(f\"Original dataset: {len(labels):,} users\")\n",
    "\n",
    "# Stratified sampling\n",
    "bot_ratio = 0.14\n",
    "n_bots = int(SAMPLE_SIZE * bot_ratio)\n",
    "n_humans = SAMPLE_SIZE - n_bots\n",
    "\n",
    "print(f\"\\nTarget sample: {SAMPLE_SIZE:,} users\")\n",
    "print(f\"  - Bots: {n_bots:,}\")\n",
    "print(f\"  - Humans: {n_humans:,}\")\n",
    "\n",
    "# Sample\n",
    "bots = labels[labels['label'] == 'bot'].sample(n=n_bots, random_state=42)\n",
    "humans = labels[labels['label'] == 'human'].sample(n=n_humans, random_state=42)\n",
    "sampled_labels = pd.concat([bots, humans]).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "sampled_labels.to_csv('final_x_dataset/sampled_labels_small.csv', index=False)\n",
    "print(f\"\\nâœ… Saved to 'sampled_labels_small.csv'\")\n",
    "\n",
    "sampled_user_ids = set(sampled_labels['id'].values)\n",
    "\n",
    "# ============================================================\n",
    "# FILTER EDGES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILTERING TWEETS FOR SMALL SAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "chunk_size = 50000\n",
    "filtered_edges = []\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv('x_dataset/edge.csv', \n",
    "                                       names=['source_id', 'relation', 'target_id'],\n",
    "                                       chunksize=chunk_size)):\n",
    "    post_chunk = chunk[(chunk['relation'] == 'post') & \n",
    "                       (chunk['source_id'].isin(sampled_user_ids))]\n",
    "    filtered_edges.append(post_chunk)\n",
    "    \n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"  Processed {(i+1)*chunk_size:,} edges...\")\n",
    "\n",
    "sampled_edges = pd.concat(filtered_edges).reset_index(drop=True)\n",
    "sampled_edges.to_csv('final_x_dataset/sampled_edges_small.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… Filtered tweets: {len(sampled_edges):,}\")\n",
    "\n",
    "# Statistics\n",
    "tweets_per_user = sampled_edges.groupby('source_id').size()\n",
    "print(f\"\\nðŸ“Š Users with tweets: {len(tweets_per_user):,}\")\n",
    "print(f\"ðŸ“Š Users with â‰¥2 tweets: {(tweets_per_user >= 2).sum():,}\")\n",
    "\n",
    "# Save final tweet IDs\n",
    "final_tweet_ids = set(sampled_edges['target_id'].values)\n",
    "with open('final_tweet_ids_small.txt', 'w') as f:\n",
    "    for tid in final_tweet_ids:\n",
    "        f.write(f\"{tid}\\n\")\n",
    "\n",
    "print(f\"âœ… Saved {len(final_tweet_ids):,} tweet IDs\")\n",
    "print(\"\\nâœ… SMALL SAMPLE READY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6211282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXTRACTING TWEETS (IJSON STREAMING - FOR VERY LARGE FILES)\n",
      "======================================================================\n",
      "\n",
      "[1/3] Loading tweet IDs to extract...\n",
      "âœ… Loaded 182,328 tweet IDs to extract\n",
      "\n",
      "[2/3] Extracting tweets with ijson (streaming parser)...\n",
      "âš ï¸ This may take 20-60 minutes for 11GB file...\n",
      "â³ Processing... (no progress bar due to streaming)\n",
      "\n",
      "  â³ Processed: 100,000 tweets, Found: 200\n",
      "\n",
      "  â³ Processed: 200,000 tweets, Found: 280\n",
      "\n",
      "  â³ Processed: 300,000 tweets, Found: 480\n",
      "\n",
      "  â³ Processed: 400,000 tweets, Found: 680\n",
      "\n",
      "  â³ Processed: 500,000 tweets, Found: 880\n",
      "\n",
      "  â³ Processed: 600,000 tweets, Found: 959\n",
      "  âœ… Found: 1,000 / 182,328 tweets (0.5%)\n",
      "  â³ Processed: 700,000 tweets, Found: 1,156\n",
      "\n",
      "  â³ Processed: 800,000 tweets, Found: 1,360\n",
      "\n",
      "  â³ Processed: 900,000 tweets, Found: 1,558\n",
      "\n",
      "  â³ Processed: 1,000,000 tweets, Found: 1,790\n",
      "\n",
      "  â³ Processed: 1,100,000 tweets, Found: 1,984\n",
      "  âœ… Found: 2,000 / 182,328 tweets (1.1%)\n",
      "  â³ Processed: 1,200,000 tweets, Found: 2,253\n",
      "\n",
      "  â³ Processed: 1,300,000 tweets, Found: 2,446\n",
      "\n",
      "  â³ Processed: 1,400,000 tweets, Found: 2,623\n",
      "\n",
      "  â³ Processed: 1,500,000 tweets, Found: 2,767\n",
      "\n",
      "  â³ Processed: 1,600,000 tweets, Found: 2,957\n",
      "  âœ… Found: 3,000 / 182,328 tweets (1.6%)\n",
      "  â³ Processed: 1,700,000 tweets, Found: 3,037\n",
      "\n",
      "  â³ Processed: 1,800,000 tweets, Found: 3,396\n",
      "\n",
      "  â³ Processed: 1,900,000 tweets, Found: 3,558\n",
      "\n",
      "  â³ Processed: 2,000,000 tweets, Found: 3,872\n",
      "\n",
      "  â³ Processed: 2,100,000 tweets, Found: 3,952\n",
      "  âœ… Found: 4,000 / 182,328 tweets (2.2%)\n",
      "  â³ Processed: 2,200,000 tweets, Found: 4,124\n",
      "\n",
      "  â³ Processed: 2,300,000 tweets, Found: 4,364\n",
      "\n",
      "  â³ Processed: 2,400,000 tweets, Found: 4,671\n",
      "\n",
      "  â³ Processed: 2,500,000 tweets, Found: 4,871\n",
      "  âœ… Found: 5,000 / 182,328 tweets (2.7%)\n",
      "  â³ Processed: 2,600,000 tweets, Found: 5,191\n",
      "\n",
      "  â³ Processed: 2,700,000 tweets, Found: 5,391\n",
      "\n",
      "  â³ Processed: 2,800,000 tweets, Found: 5,590\n",
      "\n",
      "  â³ Processed: 2,900,000 tweets, Found: 5,829\n",
      "\n",
      "  â³ Processed: 3,000,000 tweets, Found: 5,949\n",
      "  âœ… Found: 6,000 / 182,328 tweets (3.3%)\n",
      "  â³ Processed: 3,100,000 tweets, Found: 6,109\n",
      "\n",
      "  â³ Processed: 3,200,000 tweets, Found: 6,149\n",
      "\n",
      "  â³ Processed: 3,300,000 tweets, Found: 6,468\n",
      "\n",
      "  â³ Processed: 3,400,000 tweets, Found: 6,748\n",
      "  âœ… Found: 7,000 / 182,328 tweets (3.8%)\n",
      "  â³ Processed: 3,500,000 tweets, Found: 7,012\n",
      "\n",
      "  â³ Processed: 3,600,000 tweets, Found: 7,256\n",
      "\n",
      "  â³ Processed: 3,700,000 tweets, Found: 7,336\n",
      "\n",
      "  â³ Processed: 3,800,000 tweets, Found: 7,536\n",
      "\n",
      "  â³ Processed: 3,900,000 tweets, Found: 7,816\n",
      "\n",
      "  â³ Processed: 4,000,000 tweets, Found: 7,936\n",
      "  âœ… Found: 8,000 / 182,328 tweets (4.4%)\n",
      "  â³ Processed: 4,100,000 tweets, Found: 8,016\n",
      "\n",
      "  â³ Processed: 4,200,000 tweets, Found: 8,176\n",
      "\n",
      "  â³ Processed: 4,300,000 tweets, Found: 8,374\n",
      "\n",
      "  â³ Processed: 4,400,000 tweets, Found: 8,614\n",
      "\n",
      "  â³ Processed: 4,500,000 tweets, Found: 8,890\n",
      "  âœ… Found: 9,000 / 182,328 tweets (4.9%)\n",
      "  â³ Processed: 4,600,000 tweets, Found: 9,050\n",
      "\n",
      "  â³ Processed: 4,700,000 tweets, Found: 9,130\n",
      "\n",
      "  â³ Processed: 4,800,000 tweets, Found: 9,290\n",
      "\n",
      "  â³ Processed: 4,900,000 tweets, Found: 9,569\n",
      "  âœ… Found: 10,000 / 182,328 tweets (5.5%)\n",
      "  â³ Processed: 5,000,000 tweets, Found: 10,039\n",
      "\n",
      "  â³ Processed: 5,100,000 tweets, Found: 10,199\n",
      "\n",
      "  â³ Processed: 5,200,000 tweets, Found: 10,319\n",
      "\n",
      "  â³ Processed: 5,300,000 tweets, Found: 10,561\n",
      "\n",
      "  â³ Processed: 5,400,000 tweets, Found: 10,642\n",
      "\n",
      "  â³ Processed: 5,500,000 tweets, Found: 10,882\n",
      "\n",
      "  â³ Processed: 5,600,000 tweets, Found: 10,980\n",
      "  âœ… Found: 11,000 / 182,328 tweets (6.0%)\n",
      "  â³ Processed: 5,700,000 tweets, Found: 11,100\n",
      "\n",
      "  â³ Processed: 5,800,000 tweets, Found: 11,271\n",
      "\n",
      "  â³ Processed: 5,900,000 tweets, Found: 11,431\n",
      "\n",
      "  â³ Processed: 6,000,000 tweets, Found: 11,671\n",
      "\n",
      "  â³ Processed: 6,100,000 tweets, Found: 11,791\n",
      "  âœ… Found: 12,000 / 182,328 tweets (6.6%)\n",
      "  â³ Processed: 6,200,000 tweets, Found: 12,098\n",
      "\n",
      "  â³ Processed: 6,300,000 tweets, Found: 12,378\n",
      "\n",
      "  â³ Processed: 6,400,000 tweets, Found: 12,578\n",
      "\n",
      "  â³ Processed: 6,500,000 tweets, Found: 12,780\n",
      "  âœ… Found: 13,000 / 182,328 tweets (7.1%)\n",
      "  â³ Processed: 6,600,000 tweets, Found: 13,015\n",
      "\n",
      "  â³ Processed: 6,700,000 tweets, Found: 13,295\n",
      "\n",
      "  â³ Processed: 6,800,000 tweets, Found: 13,414\n",
      "\n",
      "  â³ Processed: 6,900,000 tweets, Found: 13,696\n",
      "\n",
      "  â³ Processed: 7,000,000 tweets, Found: 13,896\n",
      "  âœ… Found: 14,000 / 182,328 tweets (7.7%)\n",
      "  â³ Processed: 7,100,000 tweets, Found: 14,134\n",
      "\n",
      "  â³ Processed: 7,200,000 tweets, Found: 14,254\n",
      "\n",
      "  â³ Processed: 7,300,000 tweets, Found: 14,497\n",
      "\n",
      "  â³ Processed: 7,400,000 tweets, Found: 14,736\n",
      "  âœ… Found: 15,000 / 182,328 tweets (8.2%)\n",
      "  â³ Processed: 7,500,000 tweets, Found: 15,016\n",
      "\n",
      "  â³ Processed: 7,600,000 tweets, Found: 15,123\n",
      "\n",
      "  â³ Processed: 7,700,000 tweets, Found: 15,366\n",
      "\n",
      "  â³ Processed: 7,800,000 tweets, Found: 15,538\n",
      "\n",
      "  â³ Processed: 7,900,000 tweets, Found: 15,777\n",
      "\n",
      "  â³ Processed: 8,000,000 tweets, Found: 15,911\n",
      "  âœ… Found: 16,000 / 182,328 tweets (8.8%)\n",
      "  â³ Processed: 8,100,000 tweets, Found: 16,196\n",
      "\n",
      "  â³ Processed: 8,200,000 tweets, Found: 16,512\n",
      "\n",
      "  â³ Processed: 8,300,000 tweets, Found: 16,671\n",
      "\n",
      "  â³ Processed: 8,400,000 tweets, Found: 16,798\n",
      "\n",
      "  â³ Processed: 8,500,000 tweets, Found: 16,918\n",
      "  âœ… Found: 17,000 / 182,328 tweets (9.3%)\n",
      "  â³ Processed: 8,600,000 tweets, Found: 17,263\n",
      "\n",
      "  â³ Processed: 8,700,000 tweets, Found: 17,423\n",
      "\n",
      "  â³ Processed: 8,800,000 tweets, Found: 17,703\n",
      "\n",
      "  â³ Processed: 8,900,000 tweets, Found: 17,855\n",
      "\n",
      "  â³ Processed: 9,000,000 tweets, Found: 17,975\n",
      "  âœ… Found: 18,000 / 182,328 tweets (9.9%)\n",
      "  â³ Processed: 9,100,000 tweets, Found: 18,193\n",
      "\n",
      "  â³ Processed: 9,200,000 tweets, Found: 18,353\n",
      "\n",
      "  â³ Processed: 9,300,000 tweets, Found: 18,633\n",
      "\n",
      "  â³ Processed: 9,400,000 tweets, Found: 18,832\n",
      "\n",
      "  â³ Processed: 9,500,000 tweets, Found: 18,969\n",
      "  âœ… Found: 19,000 / 182,328 tweets (10.4%)\n",
      "  â³ Processed: 9,600,000 tweets, Found: 19,248\n",
      "\n",
      "  â³ Processed: 9,700,000 tweets, Found: 19,488\n",
      "\n",
      "  â³ Processed: 9,800,000 tweets, Found: 19,884\n",
      "  âœ… Found: 20,000 / 182,328 tweets (11.0%)\n",
      "  â³ Processed: 9,900,000 tweets, Found: 20,044\n",
      "\n",
      "  â³ Processed: 10,000,000 tweets, Found: 20,123\n",
      "\n",
      "\n",
      "âœ… Extraction complete!\n",
      "   Tweets processed: 10,000,000\n",
      "   Tweets found: 20,123\n",
      "   Coverage: 11.0%\n",
      "\n",
      "[3/3] Converting to DataFrame and saving...\n",
      "âœ… DataFrame created: (20123, 12)\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION STATISTICS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Tweets extracted: 20,123\n",
      "ðŸ“Š Unique users: 640\n",
      "\n",
      "ðŸ“Š Tweets per user:\n",
      "   Mean:   31.4\n",
      "   Median: 40\n",
      "   Min:    1\n",
      "   Max:    102\n",
      "\n",
      "ðŸ“Š Users with â‰¥2 tweets: 582 (90.9%)\n",
      "\n",
      "ðŸ“‹ Sample:\n",
      "               tweet_id  user_id                created_at  \\\n",
      "0  t1499088999486525442   257593 2022-03-02 18:27:23+00:00   \n",
      "1  t1499089384590782464   257593 2022-03-02 18:28:55+00:00   \n",
      "2  t1499089844752031749   257593 2022-03-02 18:30:45+00:00   \n",
      "3  t1499090442004144129   257593 2022-03-02 18:33:07+00:00   \n",
      "4  t1499095253453881349   257593 2022-03-02 18:52:14+00:00   \n",
      "\n",
      "                                                text  retweet_count  \\\n",
      "0                        @hamzattar Ù‡Ù‡Ù‡Ù‡Ù‡ Ù„ÙŠØ´ Ù‡ÙŠÙƒØŸØŸØŸ              0   \n",
      "1  @hamzattar Ø§ÙˆÙˆÙˆÙˆÙØ± Ø§ÙƒØªØ± Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ù„ Ù‡Ù‡Ù‡Ù‡Ù‡ Ø¨Ø³ ÙˆØ§Ù„...              0   \n",
      "2  @hamzattar Ù…Ø¹ Ø´ÙˆÙŠØ© Sam Smith Ùˆ Ø§ÙƒÙŠÙ†Ø© Ø±Ø²Ù‚ Ùˆ Ø¯Ø±Ø§...              0   \n",
      "3                          @hamzattar Ù‚ØªÙ„ØªÙ†ÙŠ ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£              0   \n",
      "4  Met with @alkanadiyya earlier today, a great p...              9   \n",
      "\n",
      "   reply_count  like_count  quote_count  mention_count  hashtag_count  \\\n",
      "0          NaN           0          NaN              1              0   \n",
      "1          NaN           1          NaN              1              0   \n",
      "2          NaN           0          NaN              1              0   \n",
      "3          NaN           0          NaN              1              0   \n",
      "4          NaN          50          NaN              2              1   \n",
      "\n",
      "   url_count  text_length  \n",
      "0          0           27  \n",
      "1          0           99  \n",
      "2          0           57  \n",
      "3          0           25  \n",
      "4          0          275  \n",
      "\n",
      "âœ… Saved to 'final_x_dataset/extracted_tweets.csv'\n",
      "\n",
      "======================================================================\n",
      "âœ… EXTRACTION COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXTRACTING TWEETS (IJSON STREAMING - FOR VERY LARGE FILES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: LOAD TWEET IDs TO EXTRACT\n",
    "# ============================================================\n",
    "print(\"\\n[1/3] Loading tweet IDs to extract...\")\n",
    "with open('final_x_dataset/final_tweet_ids_small.txt', 'r') as f:\n",
    "    final_tweet_ids = set(line.strip() for line in f)\n",
    "print(f\"âœ… Loaded {len(final_tweet_ids):,} tweet IDs to extract\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: STREAMING EXTRACTION WITH IJSON\n",
    "# ============================================================\n",
    "print(\"\\n[2/3] Extracting tweets with ijson (streaming parser)...\")\n",
    "print(\"âš ï¸ This may take 20-60 minutes for 11GB file...\")\n",
    "print(\"â³ Processing... (no progress bar due to streaming)\")\n",
    "\n",
    "file_path = 'x_dataset/tweet_0.json'\n",
    "extracted_tweets = []\n",
    "tweets_processed = 0\n",
    "tweets_found = 0\n",
    "\n",
    "def extract_tweet_data(tweet):\n",
    "    \"\"\"Extract relevant data from a tweet object\"\"\"\n",
    "    try:\n",
    "        entities = tweet.get('entities', {})\n",
    "        mentions = entities.get('user_mentions', [])\n",
    "        hashtags = entities.get('hashtags', [])\n",
    "        urls = entities.get('urls', [])\n",
    "        \n",
    "        return {\n",
    "            'tweet_id': tweet.get('id'),\n",
    "            'user_id': tweet.get('author_id'),\n",
    "            'created_at': tweet.get('created_at'),\n",
    "            'text': tweet.get('text', ''),\n",
    "            'retweet_count': tweet.get('public_metrics', {}).get('retweet_count', 0),\n",
    "            'reply_count': tweet.get('public_metrics', {}).get('reply_count', 0),\n",
    "            'like_count': tweet.get('public_metrics', {}).get('like_count', 0),\n",
    "            'quote_count': tweet.get('public_metrics', {}).get('quote_count', 0),\n",
    "            'mention_count': len(mentions) if mentions else 0,\n",
    "            'hashtag_count': len(hashtags) if hashtags else 0,\n",
    "            'url_count': len(urls) if urls else 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Open file with ijson (streaming mode)\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Parse as array items (each tweet is an item in the array)\n",
    "    parser = ijson.items(f, 'item')\n",
    "    \n",
    "    for tweet in parser:\n",
    "        tweets_processed += 1\n",
    "        \n",
    "        # Check if this tweet is in our list\n",
    "        tweet_id = tweet.get('id')\n",
    "        if tweet_id in final_tweet_ids:\n",
    "            tweet_data = extract_tweet_data(tweet)\n",
    "            if tweet_data:\n",
    "                extracted_tweets.append(tweet_data)\n",
    "                tweets_found += 1\n",
    "                \n",
    "                # Progress update every 1000 tweets\n",
    "                if tweets_found % 1000 == 0:\n",
    "                    print(f\"\\r  âœ… Found: {tweets_found:,} / {len(final_tweet_ids):,} tweets ({tweets_found/len(final_tweet_ids)*100:.1f}%)\", end='', flush=True)\n",
    "        \n",
    "        # Progress update every 100k tweets processed\n",
    "        if tweets_processed % 100000 == 0:\n",
    "            print(f\"\\n  â³ Processed: {tweets_processed:,} tweets, Found: {tweets_found:,}\", flush=True)\n",
    "            gc.collect()\n",
    "        \n",
    "        # Early stop if found all\n",
    "        if tweets_found == len(final_tweet_ids):\n",
    "            print(f\"\\nâœ… Found all tweets! Stopping early.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n\\nâœ… Extraction complete!\")\n",
    "print(f\"   Tweets processed: {tweets_processed:,}\")\n",
    "print(f\"   Tweets found: {tweets_found:,}\")\n",
    "print(f\"   Coverage: {tweets_found / len(final_tweet_ids) * 100:.1f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: CONVERT TO DATAFRAME & SAVE\n",
    "# ============================================================\n",
    "print(\"\\n[3/3] Converting to DataFrame and saving...\")\n",
    "\n",
    "if len(extracted_tweets) == 0:\n",
    "    print(\"\\nâŒ ERROR: No tweets extracted!\")\n",
    "    raise ValueError(\"No tweets found - check ID format or file structure\")\n",
    "\n",
    "tweets_df = pd.DataFrame(extracted_tweets)\n",
    "tweets_df['created_at'] = pd.to_datetime(tweets_df['created_at'])\n",
    "tweets_df = tweets_df.sort_values(['user_id', 'created_at']).reset_index(drop=True)\n",
    "tweets_df['text_length'] = tweets_df['text'].str.len()\n",
    "\n",
    "print(f\"âœ… DataFrame created: {tweets_df.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTRACTION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Tweets extracted: {len(tweets_df):,}\")\n",
    "print(f\"ðŸ“Š Unique users: {tweets_df['user_id'].nunique():,}\")\n",
    "\n",
    "tweets_per_user = tweets_df.groupby('user_id').size()\n",
    "print(f\"\\nðŸ“Š Tweets per user:\")\n",
    "print(f\"   Mean:   {tweets_per_user.mean():.1f}\")\n",
    "print(f\"   Median: {tweets_per_user.median():.0f}\")\n",
    "print(f\"   Min:    {tweets_per_user.min()}\")\n",
    "print(f\"   Max:    {tweets_per_user.max()}\")\n",
    "\n",
    "users_with_gte2 = (tweets_per_user >= 2).sum()\n",
    "print(f\"\\nðŸ“Š Users with â‰¥2 tweets: {users_with_gte2:,} ({users_with_gte2/len(tweets_per_user)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Sample:\")\n",
    "print(tweets_df.head())\n",
    "\n",
    "# ============================================================\n",
    "# SAVE\n",
    "# ============================================================\n",
    "output_file = 'final_x_dataset/extracted_tweets.csv'\n",
    "tweets_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ… Saved to '{output_file}'\")\n",
    "\n",
    "del extracted_tweets, tweets_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… EXTRACTION COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa8a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING FINAL DATASET (1 CSV FILE - LIKE OLD DATASET)\n",
      "======================================================================\n",
      "\n",
      "[1/4] Loading extracted tweets...\n",
      "âœ… Loaded 20,123 tweets\n",
      "   user_id type: object\n",
      "\n",
      "[2/4] Loading user metadata from user.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [00:12<00:00, 82819.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1,000,000 users\n",
      "   user_id type: object\n",
      "\n",
      "[3/4] Loading labels...\n",
      "   label user_id type: object\n",
      "âœ… Loaded 20,000 labels\n",
      "\n",
      "ðŸ”§ Fixing data types...\n",
      "âœ… user_id standardized to string type\n",
      "\n",
      "[4/4] Filtering and merging data...\n",
      "   Filtering users_df to 640 sampled users...\n",
      "âœ… Filtered users_df: 640 users\n",
      "\n",
      "Merging datasets...\n",
      "âœ… Merged with user metadata: (20123, 17)\n",
      "âœ… Merged with labels: (20123, 18)\n",
      "\n",
      "Renaming columns to match old dataset format...\n",
      "âœ… Columns selected: 11\n",
      "\n",
      "======================================================================\n",
      "DATA VALIDATION & CLEANING\n",
      "======================================================================\n",
      "\n",
      "Before cleaning: (20123, 11)\n",
      "\n",
      "ðŸ“Š Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "After removing missing critical data: (20123, 11)\n",
      "âœ… Data types converted\n",
      "\n",
      "======================================================================\n",
      "FINAL DATASET STATISTICS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Dataset shape: (20123, 11)\n",
      "   Total tweets: 20,123\n",
      "   Unique users: 640\n",
      "\n",
      "ðŸ“Š Tweets per user:\n",
      "   Mean:   31.4\n",
      "   Median: 40\n",
      "   Min:    1\n",
      "   Max:    102\n",
      "\n",
      "ðŸ“Š Users with â‰¥2 tweets: 582 (90.9%)\n",
      "\n",
      "ðŸ“Š User labels:\n",
      "   Bots (1):   57 (8.9%)\n",
      "   Humans (0): 583 (91.1%)\n",
      "\n",
      "ðŸŽ¯ Comparison with literature:\n",
      "   vs Perdana et al. (56 users):  10.4x larger\n",
      "   vs Aditya et al. (39 users):   14.9x larger\n",
      "   vs Priyatno et al. (32 users): 18.2x larger\n",
      "\n",
      "ðŸ“‹ Sample data (first 5 rows):\n",
      "   User ID                 Created At  \\\n",
      "0  u257593  2022-03-02 18:27:23+00:00   \n",
      "1  u257593  2022-03-02 18:28:55+00:00   \n",
      "2  u257593  2022-03-02 18:30:45+00:00   \n",
      "3  u257593  2022-03-02 18:33:07+00:00   \n",
      "4  u257593  2022-03-02 18:52:14+00:00   \n",
      "\n",
      "                                               Tweet  Retweet Count  \\\n",
      "0                        @hamzattar Ù‡Ù‡Ù‡Ù‡Ù‡ Ù„ÙŠØ´ Ù‡ÙŠÙƒØŸØŸØŸ              0   \n",
      "1  @hamzattar Ø§ÙˆÙˆÙˆÙˆÙØ± Ø§ÙƒØªØ± Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ù„ Ù‡Ù‡Ù‡Ù‡Ù‡ Ø¨Ø³ ÙˆØ§Ù„...              0   \n",
      "2  @hamzattar Ù…Ø¹ Ø´ÙˆÙŠØ© Sam Smith Ùˆ Ø§ÙƒÙŠÙ†Ø© Ø±Ø²Ù‚ Ùˆ Ø¯Ø±Ø§...              0   \n",
      "3                          @hamzattar Ù‚ØªÙ„ØªÙ†ÙŠ ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£              0   \n",
      "4  Met with @alkanadiyya earlier today, a great p...              9   \n",
      "\n",
      "   Mention Count  Follower Count  Verified  Bot Label  Hashtag Count  \\\n",
      "0              1          105469         1          0              0   \n",
      "1              1          105469         1          0              0   \n",
      "2              1          105469         1          0              0   \n",
      "3              1          105469         1          0              0   \n",
      "4              2          105469         1          0              1   \n",
      "\n",
      "   URL Count  Tweet Length  \n",
      "0          0            27  \n",
      "1          0            99  \n",
      "2          0            57  \n",
      "3          0            25  \n",
      "4          0           275  \n",
      "\n",
      "ðŸ“‹ Column info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20123 entries, 0 to 20122\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   User ID         20123 non-null  object\n",
      " 1   Created At      20123 non-null  object\n",
      " 2   Tweet           20123 non-null  object\n",
      " 3   Retweet Count   20123 non-null  int64 \n",
      " 4   Mention Count   20123 non-null  int64 \n",
      " 5   Follower Count  20123 non-null  int64 \n",
      " 6   Verified        20123 non-null  int64 \n",
      " 7   Bot Label       20123 non-null  int64 \n",
      " 8   Hashtag Count   20123 non-null  int64 \n",
      " 9   URL Count       20123 non-null  int64 \n",
      " 10  Tweet Length    20123 non-null  int64 \n",
      "dtypes: int64(8), object(3)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "\n",
      "âœ… Saved to 'final_x_dataset/bot_detection_data_1.csv'\n",
      "\n",
      "======================================================================\n",
      "âœ… DATASET PREPARATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ Ready for modeling!\n",
      "   File: final_x_dataset/bot_detection_data_1.csv\n",
      "   Format: SAME as old dataset (multiple tweets per user)\n",
      "   Can use: EXACT same code flow as before\n",
      "\n",
      "ðŸ“ File size: 9.78 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING FINAL DATASET (1 CSV FILE - LIKE OLD DATASET)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: LOAD ALL DATA\n",
    "# ============================================================\n",
    "print(\"\\n[1/4] Loading extracted tweets...\")\n",
    "tweets_df = pd.read_csv('final_x_dataset/extracted_tweets.csv')\n",
    "# Setelah load tweets_df, tambahkan ini:\n",
    "tweets_df['user_id'] = 'u' + tweets_df['user_id'].astype(str)\n",
    "print(f\"âœ… Loaded {len(tweets_df):,} tweets\")\n",
    "print(f\"   user_id type: {tweets_df['user_id'].dtype}\")\n",
    "\n",
    "print(\"\\n[2/4] Loading user metadata from user.json...\")\n",
    "with open('x_dataset/user.json', 'r', encoding='utf-8') as f:\n",
    "    users_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "users_list = []\n",
    "for user in tqdm(users_data, desc=\"Processing users\"):\n",
    "    users_list.append({\n",
    "        'user_id': user['id'],  # Keep as string from JSON\n",
    "        'followers_count': user['public_metrics']['followers_count'],\n",
    "        'following_count': user['public_metrics']['following_count'],\n",
    "        'user_tweet_count': user['public_metrics']['tweet_count'],\n",
    "        'verified': user['verified'],\n",
    "        'account_created_at': user['created_at']\n",
    "    })\n",
    "\n",
    "users_df = pd.DataFrame(users_list)\n",
    "print(f\"âœ… Loaded {len(users_df):,} users\")\n",
    "print(f\"   user_id type: {users_df['user_id'].dtype}\")\n",
    "\n",
    "print(\"\\n[3/4] Loading labels...\")\n",
    "labels_df = pd.read_csv('final_x_dataset/sampled_labels.csv')\n",
    "labels_df.columns = ['user_id', 'label']\n",
    "print(f\"   label user_id type: {labels_df['user_id'].dtype}\")\n",
    "\n",
    "# Convert label to binary: bot=1, human=0\n",
    "labels_df['Bot_Label'] = labels_df['label'].map({'bot': 1, 'human': 0})\n",
    "print(f\"âœ… Loaded {len(labels_df):,} labels\")\n",
    "\n",
    "# ============================================================\n",
    "# FIX DATA TYPES - CONVERT ALL user_id TO STRING\n",
    "# ============================================================\n",
    "print(\"\\nðŸ”§ Fixing data types...\")\n",
    "\n",
    "# Convert all user_id columns to string for consistent merging\n",
    "tweets_df['user_id'] = tweets_df['user_id'].astype(str)\n",
    "users_df['user_id'] = users_df['user_id'].astype(str)\n",
    "labels_df['user_id'] = labels_df['user_id'].astype(str)\n",
    "\n",
    "print(f\"âœ… user_id standardized to string type\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: FILTER users_df TO ONLY SAMPLED USERS (REDUCE MEMORY)\n",
    "# ============================================================\n",
    "print(\"\\n[4/4] Filtering and merging data...\")\n",
    "\n",
    "# Get list of users we actually need (from tweets)\n",
    "sampled_user_ids = set(tweets_df['user_id'].unique())\n",
    "print(f\"   Filtering users_df to {len(sampled_user_ids):,} sampled users...\")\n",
    "\n",
    "# Filter users_df to only include sampled users\n",
    "users_df = users_df[users_df['user_id'].isin(sampled_user_ids)].copy()\n",
    "print(f\"âœ… Filtered users_df: {len(users_df):,} users\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: MERGE ALL DATA (LIKE OLD DATASET FORMAT!)\n",
    "# ============================================================\n",
    "print(\"\\nMerging datasets...\")\n",
    "\n",
    "# Merge tweets with user metadata\n",
    "df = tweets_df.merge(users_df, on='user_id', how='left')\n",
    "print(f\"âœ… Merged with user metadata: {df.shape}\")\n",
    "\n",
    "# Merge with labels\n",
    "df = df.merge(labels_df[['user_id', 'Bot_Label']], on='user_id', how='left')\n",
    "print(f\"âœ… Merged with labels: {df.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: RENAME & REORDER COLUMNS (MATCH OLD DATASET FORMAT)\n",
    "# ============================================================\n",
    "print(\"\\nRenaming columns to match old dataset format...\")\n",
    "\n",
    "# Rename columns to match old dataset\n",
    "df_final = df.rename(columns={\n",
    "    'user_id': 'User ID',\n",
    "    'created_at': 'Created At',\n",
    "    'text': 'Tweet',\n",
    "    'retweet_count': 'Retweet Count',\n",
    "    'mention_count': 'Mention Count',\n",
    "    'followers_count': 'Follower Count',\n",
    "    'verified': 'Verified',\n",
    "    'hashtag_count': 'Hashtag Count',\n",
    "    'url_count': 'URL Count',\n",
    "    'text_length': 'Tweet Length'\n",
    "}).copy()\n",
    "\n",
    "# Ensure Bot Label column exists\n",
    "if 'Bot_Label' not in df_final.columns:\n",
    "    print(\"âš ï¸ Warning: Bot_Label not found after merge!\")\n",
    "    df_final['Bot Label'] = pd.NA\n",
    "else:\n",
    "    df_final['Bot Label'] = df_final['Bot_Label']\n",
    "\n",
    "# Select columns to keep (only those that exist)\n",
    "available_columns = [\n",
    "    'User ID',\n",
    "    'Created At',\n",
    "    'Tweet',\n",
    "    'Retweet Count',\n",
    "    'Mention Count',\n",
    "    'Follower Count',\n",
    "    'Verified',\n",
    "    'Bot Label',\n",
    "    'Hashtag Count',\n",
    "    'URL Count',\n",
    "    'Tweet Length'\n",
    "]\n",
    "\n",
    "# Filter to only existing columns\n",
    "final_columns = [col for col in available_columns if col in df_final.columns]\n",
    "df_final = df_final[final_columns]\n",
    "\n",
    "print(f\"âœ… Columns selected: {len(final_columns)}\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: DATA CLEANING & VALIDATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA VALIDATION & CLEANING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBefore cleaning: {df_final.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nðŸ“Š Missing values:\")\n",
    "missing = df_final.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# Remove rows with missing critical data\n",
    "critical_cols = ['User ID', 'Created At']\n",
    "if 'Bot Label' in df_final.columns:\n",
    "    critical_cols.append('Bot Label')\n",
    "\n",
    "df_final = df_final.dropna(subset=critical_cols)\n",
    "print(f\"\\nAfter removing missing critical data: {df_final.shape}\")\n",
    "\n",
    "# Convert data types\n",
    "df_final['Verified'] = df_final['Verified'].astype(int)\n",
    "if 'Bot Label' in df_final.columns:\n",
    "    df_final['Bot Label'] = df_final['Bot Label'].astype(int)\n",
    "\n",
    "print(f\"âœ… Data types converted\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset shape: {df_final.shape}\")\n",
    "print(f\"   Total tweets: {len(df_final):,}\")\n",
    "print(f\"   Unique users: {df_final['User ID'].nunique():,}\")\n",
    "\n",
    "# Tweets per user distribution\n",
    "tweets_per_user = df_final.groupby('User ID').size()\n",
    "print(f\"\\nðŸ“Š Tweets per user:\")\n",
    "print(f\"   Mean:   {tweets_per_user.mean():.1f}\")\n",
    "print(f\"   Median: {tweets_per_user.median():.0f}\")\n",
    "print(f\"   Min:    {tweets_per_user.min()}\")\n",
    "print(f\"   Max:    {tweets_per_user.max()}\")\n",
    "\n",
    "# Users with â‰¥2 tweets (can calculate TIE)\n",
    "users_gte2 = (tweets_per_user >= 2).sum()\n",
    "print(f\"\\nðŸ“Š Users with â‰¥2 tweets: {users_gte2:,} ({users_gte2/len(tweets_per_user)*100:.1f}%)\")\n",
    "\n",
    "# Label distribution\n",
    "if 'Bot Label' in df_final.columns:\n",
    "    label_counts = df_final.groupby('User ID')['Bot Label'].first().value_counts()\n",
    "    print(f\"\\nðŸ“Š User labels:\")\n",
    "    print(f\"   Bots (1):   {label_counts.get(1, 0):,} ({label_counts.get(1, 0)/label_counts.sum()*100:.1f}%)\")\n",
    "    print(f\"   Humans (0): {label_counts.get(0, 0):,} ({label_counts.get(0, 0)/label_counts.sum()*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Comparison with literature:\")\n",
    "    print(f\"   vs Perdana et al. (56 users):  {users_gte2/56:.1f}x larger\")\n",
    "    print(f\"   vs Aditya et al. (39 users):   {users_gte2/39:.1f}x larger\")\n",
    "    print(f\"   vs Priyatno et al. (32 users): {users_gte2/32:.1f}x larger\")\n",
    "\n",
    "# Sample data\n",
    "print(\"\\nðŸ“‹ Sample data (first 5 rows):\")\n",
    "print(df_final.head())\n",
    "\n",
    "print(\"\\nðŸ“‹ Column info:\")\n",
    "print(df_final.info())\n",
    "\n",
    "# ============================================================\n",
    "# STEP 7: SAVE FINAL DATASET\n",
    "# ============================================================\n",
    "output_file = 'final_x_dataset/bot_detection_data_1.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ… Saved to '{output_file}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… DATASET PREPARATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸŽ¯ Ready for modeling!\")\n",
    "print(f\"   File: {output_file}\")\n",
    "print(f\"   Format: SAME as old dataset (multiple tweets per user)\")\n",
    "print(f\"   Can use: EXACT same code flow as before\")\n",
    "print(f\"\\nðŸ“ File size: {df_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a47ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INFORMASI DATASET AWAL\n",
      "============================================================\n",
      "Shape awal: (20123, 11)\n",
      "\n",
      "Distribusi Bot Label:\n",
      "Bot Label\n",
      "0    18173\n",
      "1     1950\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Persentase:\n",
      "Bot Label\n",
      "0    90.309596\n",
      "1     9.690404\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Jumlah Bot (Label 1): 1950\n",
      "Jumlah Non-Bot (Label 0): 18173\n",
      "\n",
      "============================================================\n",
      "METODE 1: UNDERSAMPLING\n",
      "============================================================\n",
      "Shape setelah balancing: (3900, 11)\n",
      "\n",
      "Distribusi Bot Label:\n",
      "Bot Label\n",
      "1    1950\n",
      "0    1950\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Persentase:\n",
      "Bot Label\n",
      "1    50.0\n",
      "0    50.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "============================================================\n",
      "METODE 2: OVERSAMPLING\n",
      "============================================================\n",
      "Shape setelah balancing: (36346, 11)\n",
      "\n",
      "Distribusi Bot Label:\n",
      "Bot Label\n",
      "1    18173\n",
      "0    18173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Persentase:\n",
      "Bot Label\n",
      "1    50.0\n",
      "0    50.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "============================================================\n",
      "METODE 3: HYBRID\n",
      "============================================================\n",
      "Shape setelah balancing: (20122, 11)\n",
      "\n",
      "Distribusi Bot Label:\n",
      "Bot Label\n",
      "1    10061\n",
      "0    10061\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Persentase:\n",
      "Bot Label\n",
      "1    50.0\n",
      "0    50.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "============================================================\n",
      "MENYIMPAN DATASET\n",
      "============================================================\n",
      "âœ“ Tersimpan: bot_detection_balanced_undersampling.csv\n",
      "âœ“ Tersimpan: bot_detection_balanced_oversampling.csv\n",
      "âœ“ Tersimpan: bot_detection_balanced_hybrid.csv\n",
      "\n",
      "============================================================\n",
      "REKOMENDASI\n",
      "============================================================\n",
      "\n",
      "1. UNDERSAMPLING: \n",
      "   - Gunakan jika dataset sangat besar dan ingin mengurangi ukuran\n",
      "   - Kehilangan data dari kelas mayoritas\n",
      "   - Dataset lebih kecil, training lebih cepat\n",
      "\n",
      "2. OVERSAMPLING:\n",
      "   - Gunakan jika data minoritas terlalu sedikit\n",
      "   - Mempertahankan semua data asli\n",
      "   - Dataset lebih besar, risk overfitting lebih tinggi\n",
      "\n",
      "3. HYBRID:\n",
      "   - Kompromi antara undersampling dan oversampling\n",
      "   - Ukuran dataset moderate\n",
      "   - Balance antara efisiensi dan informasi\n",
      "\n",
      "Untuk bot detection, biasanya UNDERSAMPLING atau HYBRID lebih baik\n",
      "karena menghindari duplikasi data yang berlebihan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Baca dataset\n",
    "df = pd.read_csv('final_x_dataset/bot_detection_data.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFORMASI DATASET AWAL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape awal: {df.shape}\")\n",
    "print(f\"\\nDistribusi Bot Label:\")\n",
    "print(df['Bot Label'].value_counts())\n",
    "print(f\"\\nPersentase:\")\n",
    "print(df['Bot Label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Pisahkan berdasarkan label\n",
    "bot_data = df[df['Bot Label'] == 1]\n",
    "non_bot_data = df[df['Bot Label'] == 0]\n",
    "\n",
    "print(f\"\\nJumlah Bot (Label 1): {len(bot_data)}\")\n",
    "print(f\"Jumlah Non-Bot (Label 0): {len(non_bot_data)}\")\n",
    "\n",
    "# ===== METODE 1: UNDERSAMPLING (Kurangi kelas mayoritas) =====\n",
    "# Ambil jumlah minimum dari kedua kelas\n",
    "min_samples = min(len(bot_data), len(non_bot_data))\n",
    "\n",
    "bot_downsampled = resample(bot_data, \n",
    "                           replace=False,\n",
    "                           n_samples=min_samples,\n",
    "                           random_state=42)\n",
    "\n",
    "non_bot_downsampled = resample(non_bot_data,\n",
    "                               replace=False,\n",
    "                               n_samples=min_samples,\n",
    "                               random_state=42)\n",
    "\n",
    "# Gabungkan dan shuffle\n",
    "balanced_df_undersampling = pd.concat([bot_downsampled, non_bot_downsampled])\n",
    "balanced_df_undersampling = balanced_df_undersampling.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METODE 1: UNDERSAMPLING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape setelah balancing: {balanced_df_undersampling.shape}\")\n",
    "print(f\"\\nDistribusi Bot Label:\")\n",
    "print(balanced_df_undersampling['Bot Label'].value_counts())\n",
    "print(f\"\\nPersentase:\")\n",
    "print(balanced_df_undersampling['Bot Label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# ===== METODE 2: OVERSAMPLING (Tambah kelas minoritas) =====\n",
    "max_samples = max(len(bot_data), len(non_bot_data))\n",
    "\n",
    "bot_upsampled = resample(bot_data,\n",
    "                        replace=True,\n",
    "                        n_samples=max_samples,\n",
    "                        random_state=42)\n",
    "\n",
    "non_bot_upsampled = resample(non_bot_data,\n",
    "                            replace=True,\n",
    "                            n_samples=max_samples,\n",
    "                            random_state=42)\n",
    "\n",
    "# Gabungkan dan shuffle\n",
    "balanced_df_oversampling = pd.concat([bot_upsampled, non_bot_upsampled])\n",
    "balanced_df_oversampling = balanced_df_oversampling.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METODE 2: OVERSAMPLING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape setelah balancing: {balanced_df_oversampling.shape}\")\n",
    "print(f\"\\nDistribusi Bot Label:\")\n",
    "print(balanced_df_oversampling['Bot Label'].value_counts())\n",
    "print(f\"\\nPersentase:\")\n",
    "print(balanced_df_oversampling['Bot Label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# ===== METODE 3: HYBRID (Kombinasi) =====\n",
    "# Tentukan target jumlah untuk masing-masing kelas (misalnya rata-rata)\n",
    "target_samples = (len(bot_data) + len(non_bot_data)) // 2\n",
    "\n",
    "bot_hybrid = resample(bot_data,\n",
    "                     replace=(len(bot_data) < target_samples),\n",
    "                     n_samples=target_samples,\n",
    "                     random_state=42)\n",
    "\n",
    "non_bot_hybrid = resample(non_bot_data,\n",
    "                         replace=(len(non_bot_data) < target_samples),\n",
    "                         n_samples=target_samples,\n",
    "                         random_state=42)\n",
    "\n",
    "# Gabungkan dan shuffle\n",
    "balanced_df_hybrid = pd.concat([bot_hybrid, non_bot_hybrid])\n",
    "balanced_df_hybrid = balanced_df_hybrid.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METODE 3: HYBRID\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape setelah balancing: {balanced_df_hybrid.shape}\")\n",
    "print(f\"\\nDistribusi Bot Label:\")\n",
    "print(balanced_df_hybrid['Bot Label'].value_counts())\n",
    "print(f\"\\nPersentase:\")\n",
    "print(balanced_df_hybrid['Bot Label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# ===== SIMPAN DATASET =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MENYIMPAN DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pilih metode yang ingin disimpan (sesuaikan dengan kebutuhan)\n",
    "balanced_df_undersampling.to_csv('bot_detection_balanced_undersampling.csv', index=False)\n",
    "print(\"âœ“ Tersimpan: bot_detection_balanced_undersampling.csv\")\n",
    "\n",
    "balanced_df_oversampling.to_csv('bot_detection_balanced_oversampling.csv', index=False)\n",
    "print(\"âœ“ Tersimpan: bot_detection_balanced_oversampling.csv\")\n",
    "\n",
    "balanced_df_hybrid.to_csv('bot_detection_balanced_hybrid.csv', index=False)\n",
    "print(\"âœ“ Tersimpan: bot_detection_balanced_hybrid.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REKOMENDASI\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. UNDERSAMPLING: \n",
    "   - Gunakan jika dataset sangat besar dan ingin mengurangi ukuran\n",
    "   - Kehilangan data dari kelas mayoritas\n",
    "   - Dataset lebih kecil, training lebih cepat\n",
    "\n",
    "2. OVERSAMPLING:\n",
    "   - Gunakan jika data minoritas terlalu sedikit\n",
    "   - Mempertahankan semua data asli\n",
    "   - Dataset lebih besar, risk overfitting lebih tinggi\n",
    "\n",
    "3. HYBRID:\n",
    "   - Kompromi antara undersampling dan oversampling\n",
    "   - Ukuran dataset moderate\n",
    "   - Balance antara efisiensi dan informasi\n",
    "\n",
    "Untuk bot detection, biasanya UNDERSAMPLING atau HYBRID lebih baik\n",
    "karena menghindari duplikasi data yang berlebihan.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
